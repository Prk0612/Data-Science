{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Set up the Google Custom Search API\n",
    "api_key = 'AIzaSyDcwidXSXzOJhmYLHjmd4BDuKjowudQPHY'\n",
    "search_engine_id = \"c2f721b6c9c3545ba\"\n",
    "\n",
    "# Search for the LinkedIn profile using the company name\n",
    "def search_linkedin_profile(company_name):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    query = f'{company_name} site:linkedin.com'\n",
    "    result = service.cse().list(q=query, cx=search_engine_id).execute()\n",
    "    items = result.get('items', [])\n",
    "    for item in items:\n",
    "        link = item.get('link', '')\n",
    "        if 'linkedin.com/company/' in link:\n",
    "            return link\n",
    "\n",
    "# Set up Selenium webdriver\n",
    "browser = webdriver.Chrome('Lead Scrapping UK/driver/chromedriver.exe')\n",
    "browser.get('https://www.linkedin.com/uas/login')\n",
    "\n",
    "file = open('config.txt')\n",
    "lines = file.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]\n",
    "\n",
    "elementID = browser.find_element(By.ID, 'username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element(By.ID, 'password')\n",
    "elementID.send_keys(password)\n",
    "\n",
    "elementID.submit()\n",
    "\n",
    "url = \"https://www.houzz.co.uk/professionals/interior-designers\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object with the response content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "script_tags = soup.find_all('script')\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for script_tag in script_tags:\n",
    "    if script_tag.has_attr('type') and script_tag['type'] == 'application/ld+json':\n",
    "        # Parse the JSON data\n",
    "        json_data = script_tag.string\n",
    "        try:\n",
    "            data = json.loads(json_data)\n",
    "\n",
    "            # Get the values of \"name\", \"telephone\", and \"address\" keys\n",
    "            name = data.get('name')\n",
    "            telephone = data.get('telephone')\n",
    "            # Get the values of 'streetAddress', 'addressLocality', 'addressRegion', 'postalCode', and 'addressCountry'\n",
    "            street_address = data.get('address', {}).get('streetAddress', '')\n",
    "            locality = data.get('address', {}).get('addressLocality', '')\n",
    "            region = data.get('address', {}).get('addressRegion', '')\n",
    "            postal_code = data.get('address', {}).get('postalCode', '')\n",
    "            country = data.get('address', {}).get('addressCountry', '')\n",
    "\n",
    "            # Concatenate the values with commas\n",
    "            address = ', '.join([street_address, locality, region, postal_code, country])\n",
    "\n",
    "            company_name = data.get('name')\n",
    "\n",
    "            # Search for the LinkedIn profile\n",
    "            linkedin_profile = search_linkedin_profile(company_name)\n",
    "            \n",
    "            if linkedin_profile:\n",
    "                linkedin_profile += '/about'\n",
    "            \n",
    "            # Skip the row if linkedin_profile is None\n",
    "            if linkedin_profile is None:\n",
    "                continue\n",
    "\n",
    "            # Visit the LinkedIn profile\n",
    "            browser.get(linkedin_profile)\n",
    "\n",
    "            SCROLL_PAUSE_TIME = 5\n",
    "            \n",
    "            time.sleep(2)\n",
    "\n",
    "            # Get scroll height\n",
    "            last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            for i in range(3):\n",
    "                # Scroll down to bottom\n",
    "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                # Wait to load page\n",
    "                time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "                # Calculate new scroll height and compare with last scroll height\n",
    "                new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "\n",
    "            src = browser.page_source\n",
    "            soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "            # Get website and company size\n",
    "            web = ''\n",
    "            comp = ''\n",
    "\n",
    "            dt_tags = soup.find_all('dt', class_='mb1 text-heading-medium')\n",
    "\n",
    "            for dt_tag in dt_tags:\n",
    "                dt_text = dt_tag.text.strip()\n",
    "                if dt_text == 'Website':\n",
    "                    dd_tag = dt_tag.find_next_sibling('dd')\n",
    "                    web = dd_tag.text.strip()\n",
    "                elif dt_text == 'Company size':\n",
    "                    dd_tag = dt_tag.find_next_sibling('dd')\n",
    "                    comp = dd_tag.text.strip()\n",
    "\n",
    "            data_list.append({'Name': company_name, 'Telephone': telephone, 'Address': address, 'LinkedIn URL': linkedin_profile, 'Website': web, 'Company size': comp})\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON data.\")\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
