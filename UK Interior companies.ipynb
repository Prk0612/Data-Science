{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ce30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set up the Google Custom Search API\n",
    "api_key = 'A***************************TU-4'\n",
    "search_engine_id = \"c**************5ba\"\n",
    "\n",
    "# Search for the LinkedIn profile using the company name\n",
    "def search_linkedin_profile(company_name):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    query = f'{company_name} site:linkedin.com'\n",
    "    result = service.cse().list(q=query, cx=search_engine_id).execute()\n",
    "    items = result.get('items', [])\n",
    "    for item in items:\n",
    "        link = item.get('link', '')\n",
    "        if 'linkedin.com/company/' in link:\n",
    "            return link\n",
    "\n",
    "num_pages = 4  # Set the number of pages you want to scrape\n",
    "increment = 15  # Set the increment value for each page\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "    page_number = (page - 1) * increment\n",
    "    url = f\"https://www.houzz.co.uk/professionals/interior-designers/p/{page_number}\"\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object with the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    script_tags = soup.find_all('script')\n",
    "\n",
    "    for script_tag in script_tags:\n",
    "        if script_tag.has_attr('type') and script_tag['type'] == 'application/ld+json':\n",
    "            # Parse the JSON data\n",
    "            json_data = script_tag.string\n",
    "            try:\n",
    "                data = json.loads(json_data)\n",
    "\n",
    "                # Get the values of \"name\", \"telephone\", and \"address\" keys\n",
    "                name = data.get('name')\n",
    "                telephone = data.get('telephone')\n",
    "                # Get the values of 'streetAddress', 'addressLocality', 'addressRegion', 'postalCode', and 'addressCountry'\n",
    "                street_address = data.get('address', {}).get('streetAddress', '')\n",
    "                locality = data.get('address', {}).get('addressLocality', '')\n",
    "                region = data.get('address', {}).get('addressRegion', '')\n",
    "                postal_code = data.get('address', {}).get('postalCode', '')\n",
    "                country = data.get('address', {}).get('addressCountry', '')\n",
    "\n",
    "                # Concatenate the values with commas\n",
    "                address = ', '.join([street_address, locality, region, postal_code, country])\n",
    "\n",
    "                company_name = data.get('name')\n",
    "\n",
    "                # Search for the LinkedIn profile\n",
    "                linkedin_profile = search_linkedin_profile(company_name)\n",
    "\n",
    "                if linkedin_profile:\n",
    "                    linkedin_profile += '/about/'\n",
    "\n",
    "                data_list.append({'Name': company_name, 'Telephone': data.get('telephone'), 'Address': address, 'LinkedIn URL': linkedin_profile})\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error decoding JSON data.\")\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Drop rows with \"second-to-none\" or None in \"LinkedIn URL\" column\n",
    "df.drop(df[(df[\"LinkedIn URL\"] == \"https://www.linkedin.com/company/second-to-none-inc/about/\") | (df[\"LinkedIn URL\"].isnull())].index, inplace=True)\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Print the DataFrame\n",
    "print(df[\"LinkedIn URL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from seleniumwire import webdriver as wired_webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    email = \"pr********************m\"\n",
    "    password = \"************k\"\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "        #options.add_argument('proxy-server=106.122.8.54:3128')\n",
    "    options.add_argument(r'--user-data-dir=C:\\Users\\Priyank\\AppData\\Local\\Google\\Chrome\\Default')\n",
    "    \n",
    "    browser = uc.Chrome(options=options)\n",
    "    \n",
    "    \n",
    "    def scrape_linkedin(url):\n",
    "        \n",
    "        browser.get(url)\n",
    "\n",
    "        SCROLL_PAUSE_TIME = 5\n",
    "        time.sleep(5)\n",
    "\n",
    "        last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        for i in range(3):\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "        web = ''\n",
    "        comp = ''\n",
    "\n",
    "        dt_tags = soup.find_all('dt', class_='mb1 text-heading-medium')\n",
    "\n",
    "        for dt_tag in dt_tags:\n",
    "            dt_text = dt_tag.text.strip()\n",
    "            if dt_text == 'Website':\n",
    "                dd_tag = dt_tag.find_next_sibling('dd')\n",
    "                web = dd_tag.text.strip()\n",
    "            elif dt_text == 'Company size':\n",
    "                dd_tag = dt_tag.find_next_sibling('dd')\n",
    "                comp = dd_tag.text.strip()\n",
    "\n",
    "        return web, comp\n",
    "\n",
    "\n",
    "# Create new columns to store the results\n",
    "df['Website'] = ''\n",
    "df['Company Size'] = ''\n",
    "\n",
    "# Iterate through each row\n",
    "for index, row in df.iterrows():\n",
    "    linkedin_url = row['LinkedIn URL']\n",
    "\n",
    "    # Skip row if LinkedIn URL is None\n",
    "    if linkedin_url is None:\n",
    "        continue\n",
    "\n",
    "    # Scrape LinkedIn page and get results\n",
    "    website, company_size = scrape_linkedin(linkedin_url)\n",
    "\n",
    "    # Store the results in the DataFrame\n",
    "    df.at[index, 'Website'] = website\n",
    "    df.at[index, 'Company Size'] = company_size\n",
    "\n",
    "    # Pause for a few seconds before scraping the next URL\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a53e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "df['Email'] = \"\"\n",
    "for index, row in df.iterrows():\n",
    "    website = row[\"Website\"]\n",
    "    if website:\n",
    "        try:\n",
    "            response = requests.get(website)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            email = soup.select_one(\"a[href^='mailto:']\")\n",
    "            if email:\n",
    "                email_id = email[\"href\"].replace(\"mailto:\", \"\")\n",
    "                df.at[index, \"Email\"] = email_id\n",
    "        except requests.exceptions.RequestException as e:\n",
    "           continue\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values to strings\n",
    "df = df.dropna(subset=['Company Size','min_employees','max_employees'])\n",
    "\n",
    "df['Company Size'] = df['Company Size'].astype(str)\n",
    "\n",
    "# Trim the data column and remove 'employees'\n",
    "df['Company Size'] = df['Company Size'].str.replace(' employees', '')\n",
    "\n",
    "# Split the data column into separate columns using the '-' separator\n",
    "split_data = df['Company Size'].str.split('-', expand=True)\n",
    "\n",
    "\n",
    "# Extract the minimum and maximum employee values separately\n",
    "df['min_employees'] = split_data[0].astype(int)\n",
    "df['max_employees'] = split_data[1].astype(int)\n",
    "\n",
    "\n",
    "df['max_employees'] = df['max_employees'].str.replace(',', '')\n",
    "df = df[df['max_employees'] >= 30]\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784af8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep as wait\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = 'scrapper.json'\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "# creds = None\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "# Call the Sheets API\n",
    "sheet = service.spreadsheets()\n",
    "result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                            range='Kosan!A3:AF').execute()\n",
    "# The ID and range of a sample spreadsheet.\n",
    "SAMPLE_SPREADSHEET_ID = '1GWkZibZY2UIjaqeyUWbB_NIlSMBrgz-ZXkAkGVA7I6c'\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "sheet.values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Kosan!A3:AF').execute()\n",
    "\n",
    "\n",
    "# sending to google sheets through API\n",
    "request = sheet.values().append(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Kosan!A3:AF',\n",
    "                                    valueInputOption='USER_ENTERED', body={'values': df.values.tolist()}).execute()\n",
    "print(f'Successfully Scrapped')\n",
    "wait(0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2d430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
